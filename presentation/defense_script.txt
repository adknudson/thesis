Introduction
When I began my masters I took the introduction to statistical computing with Dr schissler. I asked him if there were any projects that I could become involved with, and he sent me the opportunity to work with psychometric data provided by Dr Jiang. Initially the project started as a data cleaning project but then turned into modeling and analysis. while cleaning the data I had to learn the intimate details of the experimental setup and the deeper theory behind psychometrics.

What are psychometrics?
psychometric experiments allow us to examine the response between the world around us and our inward perceptions biological and contextual factors influence the integration or segregation of multi-sensory signals and this is important in tasks such as visual speech synthesis a psychometric function relates an observer's performance to an independent variable usually some stimulus intensity and when the temporal delay between stimuli grows large enough the brain segregates the two signals and temporal order can be determined.

Some more background
The range of temporal delays for which multisensory signals are integrated into a global percept is called the temporal binding window perceptual synchrony is the temporal delay between two signals at which an observer is unsure about their temporal order The ability to discriminate the timing of multiple sensory signals is referred to as temporal sensitivity and a deficit in temporal sensitivity may lead to a widening of the temporal binding window.

Temporal order judgment task
I now present to you what is called a temporal order judgment task which is a type of psychometric experiment we're participants are asked to determine the temporal order of two stimuli separated by a temporal delay. there are four different temporal order judgment tasks within the study that explore the relationship between different sensory signals. There are three age groups young adult middle age adult and older adult with 15 subjects within each age group.

Temporal recalibration
perceptual synchrony and temporal sensitivity can be modified through a baseline understanding. in order to perceive physical events as simultaneous our brains must adjust for differences in temporal delays of transmission of both physical signals and sensory processing. in some cases such as with audio visual stimuli the perception of simultaneity can be modified by repeatedly presenting the audiovisual stimuli at fix time separations  to an observer. this repetition of presenting the adapter stimulus is called temporal recalibration.

Experimental motivation
he would like to understand how age and tempo recalibration interact together to affect the temporal binding window The temporal binding window is described by two measures. The point of subjective simultaneity which is the temporal delay between two signals at which an observer is unsure about their temporal order, and the just noticeable difference which is the small slaps in time so that a temporal order can just be determined.

The psychometric function
The psychometric function relates a stimulus intensity with an observer's performance. as an example in the audiovisual temporal order judgment task subjects are asked to determine whether an audible tone came before a visual flash.  Positive stimulus onset asynchronous indicate that The audible tone came first, and as the temporal delay grows larger in the positive direction, an observer will give more audio first responses. The point of subjective simultaneity is to find this when the subject is doing no better than random guessing or when the response probability is 50%. The just noticeable difference is the extra temporal delay needed so that the temporal order can just be determined. historically this is defined as the difference between the 84% level and the 50% level, though the upper level depends on domain expertise.

Three ways to visualize the data
There are three natural ways of visualizing the observed data on the left we have the recorded responses applied against the stimulus onset of synchronies. It is difficult to determine if there is any type of pattern. when we add in transparency we can see that there's a higher density of positive responses for positive temporal delays and a higher density of negative responses for negative temporal delays. when a temporal delay is presented repeatedly throughout an experimental block the responses can be aggregated into binomial counts and with the binomial counts the proportion of positive responses can be derived. When the proportion is plotted against the temporal delay we can see the pattern of increasing proportion as the temporal delay goes for more negative to more positive.

Data quirks
When looking at the subjects specific proportion of responses, we came across some problematic response sets. On the left I show the responses for an older subject during the audio visual task in the post adaptation block. nearly all of their responses are positive even for very large negative temporal delays. It is very unlikely that these are genuine positive responses given the subject's performance on the other tasks. for that reason the subjects post adaptation block was removed from the data analysis. On the right we have the response proportion for subject in the visual task. Some subjects have exceptional performance and the responses approach perfect separation. In the context the perfect separation the resulting psychometric function may not be able to be estimated properly.

GLM
Because the responses arise from a Bernoulli process, the first thought might be to use a simple generalized linear model which can be fit easily in R. The problem with using a simple GLM is that estimating confidence intervals for psychometric properties requires bootstrap methods and near perfect separation results and unreliably large standard errors. One solution is to pull subjects responses together at the age group level, but then we lose out on subject-specific inferences.

MLM
The next step up from a generalized linear model is a generalized linear mixed effects model, or multi-level model. In a multi-level model, we can estimate the fixed effects shared across all age groups and subjects as well as the group level effects. when there is more variation within the group the group level estimates are shrunk towards the fixed estimates. This can greatly improve inferences when there is perfect separation. However mixed effects models using maximum likelihood are not flexible enough to model more complex psychometric features like lapse rates which I will discuss shortly. Additionally domain experts have information about psychometric quantities, and we would like to be able to incorporate that prior information into the model fitting process.

Bayesian statistics
In maximum likelihood estimation we find a parameter value that maximizes the likelihood of The observed data. Bayesian statistics is similar except we are able to also explicitly state what our prior expectation is about that parameters distribution before seeing the data. Base theorem is a simple rearranging of conditional probability The prior is some distribution over the parameter space and the likelihood of the data is the probability of an outcome and the sample space given a value in the parameter space. The posterior is our updated belief of the distribution of the parameter after having been given the data.

Baseball has a good example of using Bayesian statistics. From past seasons we know that batting averages are around 20 to 30%. When a rookie bats his first game and has an 80% batting average we don't readily believe that his true average is 80%. We use our prior information to weigh the data we observer against what we already believe.

More bayes
The integral in the denominator is the likelihood of the data averaged over the perimeter space, and it evaluates to a constant that ensures that the posterior distribution is a proper probability distribution. In other words it scales the posterior distribution so that the area under the posterior is one. Often the integral of the denominator is complex or a high dimension in which case it either cannot be analytically solved or numerically approximated. a solution is to use Markov chain Monte Carlo simulations to draw samples directly from the posterior distribution.

HMC
Markov chain Monte Carlo constructs a sequence of proposal points from a target distribution and points are accepted or rejected based off of some criterion. Hamiltonian Monte Carlo is a variant of Markov chain Monte Carlo that tracks the position of a particle following frictionless hamiltonian dynamics. Hamiltonian Monte Carlo allows for effective sampling of high-dimensional probability spaces has greatly reduced pointwise auto correlation compared to simpler markov chain Monte Carlo samplers but requires evaluating the gradient at each step. here's a demonstration of a frictionless particle sampling from a posterior distribution. The particle is imparted with random direction and momentum, and stops after a certain number of steps. The stopping point is the proposal. Higher density regions of the posterior will be explored more often.

Stan
stand as a state-of-the-art platform for statistical modeling that gives full Bayesian statistical inference with Markov chain Monte Carlo sampling a utilizes the no you turn sampling variant of hamiltonian Monte Carlo and has interfaces the popular programming language such as our Python and Julia. It provides differential probability functions and linear algebra that allows for the computation of the gradient at each step within the simulation.

Principled Bayesian workflow
a principal Bayesian workflow is a method of employing domain expertise and statistical knowledge to iteratively build a statistical model. The motivation is not to check the goodness of fit but rather to ensure that the model satisfies the contraints and goals set forth by the researcher. Michael Bettencourt suggests there are four questions which we can evaluate a model by and those are domain expertise consistency  computational faithfulness inferential adequacy and model adequacy. For inferential adequacy we ask if our inferences provide enough information to answer our questions, and for model adequacy is our model rich enough to capture the relevant structure of the true data generating process.

Workflow in brief
there are three phases to the workflow. In the pre-model pre-data phase we perform a conceptual analysis of the experiment and define the observational space. in the post model pre-data phase we turn our conceptual analysis and observational space into a mathematical model and check the prior distribution for domain expertise consistency. When we are sure that our model is consistent and that our computational tools are faithful then we move on to the postmodel post data phase where we fit the observed data to the model and then perform posterior predictive checks. we continue iterating the model until it satisfies the four evaluation questions.

Pre workflow considerations
Psychometric properties like perceptual synchrony and temporal sensitivity are estimated from the psychometric function. We have prior information about the point of subjective simultaneity and the just noticeable difference and we can use them to inform the parameters of the psychometric function. in the model building process we have choice over the psychometric function choice of linear perimeterization and choice of priors.

Choice of PF
For temporal order judgment tasks A psychometric function is a non decreasing continuous function bounded between 0 and 1. Three common choices include the gaussian cumulative distribution function, the logistic function, and the weibull cumulative distribution function. Our temporal delays span both positive and negative values, and the gaussian CDF and the logistic function have the same support as the independent variable. In practice the gaussian CDF and the logistic function result in similar inferences, and because the logistic function is the canonical choice for binomial regression we choose that for our psychometric function.

Linear parameterization
linear parameterization does not affect inferential results but the choice of parameterization can make setting priors easier. The point of subjective simultaneity and the just noticeable difference are defined as follows. The point of subjective simultaneity is defined as the temporal delay where the response probability is 50%, and the just noticeable difference is the difference between the 84% level and the 50% level. if we use the slope intercept linear perimeterization then that implies that the point of subjective simultaneity is The negative intercept divided by the slope, and the just noticeable difference is the log-odds of 0.84 level divided by the slope. if instead we use the slope location linear parameterization then the point of subjective simultaneity will be equal to the location parameter and the just noticeable difference will remain as the log odds of 0.84 divided by the slope. In this parameterization we note that the point of subjective simultaneity depends only on the location and the just noticeable difference depends just on the slope.

Selecting PSS prior
The point of subjective simultaneity can either be positive or negative some studies suggest that the separation between stimuli need to be as little as 20 milliseconds for subjects to be able to determine temporal order other studies suggest that our brains can detect temporal differences as small as 30 milliseconds we should be skeptical of point of subjective simultaneous estimates larger than say 150 milliseconds in absolute value just as a conservative estimate. This prior reflect our belief in perceptual synchrony

Selecting JND prior
it is impossible that a properly conducted block would result in adjust noticeable difference less than zero, meaning that the psychometric function is always non-decreasing. It is also unlikely that the just noticeable difference would be more than one second. this prior information is based on my own self as I can reliably detect the temporal order of two stimuli separated by one second. For example, some studies show that an input lag is small as 100 milliseconds can impair a person's typing ability and this informs our prior beliefs. The log normal distribution is convenient choice for the just noticeable difference because it is supported on the positive real numbers and has a property where the reciprocal of a log normal distribution is also a log normal distribution. We can choose the parameters so that the prior median is around 100 milliseconds and the 99% of the probability mass is below 1 second.  The slope parameter follows from the definition of the just noticeable difference.

Observational space
The responses that subjects give during a temporal order judgment task is recorded as a zero or a one if the stimulus onset a synchronies are fixed then we have repeated measurements for each temporal delay. The repeated measurements can then be aggregated into binomial counts and the number of positive responses, "k" , is distributed as a binomial distribution parameterized by "n"  trials and some probability, "p".

Iteration one
In the first iteration of the model building process we begin with a baseline model that broadly captures the structure of the observed data. It is a simple slope location that defines the likelihood of the binomial counts and the prior information of the slope and the location. Below I show how the resulting model would be written in the language of Stan.

Prior distribution of psychometric functions
Here's a figure showing the prior distributions of psychometric functions defined by the slope and location priors. These priors allow for a range of very shallow to very steep slopes that cover the 50% level estimates anywhere between -150 milliseconds to 150 milliseconds.

Iteration two
In the next generation of the model we added in the age group and block categorical variables. There are three ways to add a categorical variable. on the left the categorical variable replaces the intercept so that each level explicitly gets its own intercept. in the center each categorical variable is modeled as having the same mean and standard deviation that is learned from the data, and is the first representation for a Bayesian multi-level model. The model on the right is the same as the center but using the non-centered parameterization this is most convenient when adding multiple categorical variables.

Non centered log normal
when adding in the categorical predictors for the slope term it's more convenient to model them as coming from a normal distribution adding them together and then taking the exponential of the sum. this is because the exponential of a normal distribution is a log normal distribution. so before when we modeled the slope is coming from a log normal distribution with parameters mean log of 3 standard deviation log of 1, we can instead draw values from a normal distribution with a mean of 3 in standard deviation of 1 and then exponentiate. using the non-centered form for other distributions also has a positive effect on the efficiency of the hamiltonian Monte Carlo sampler.

Iteration two posterior checks
The left hand figure shows the trace plots for the mark of chains. they look like fuzzy caterpillars which indicates that the chains are healthy and sampling the posterior efficiently. By healthy we mean that they look like they have converge to a common posterior distribution. There is a statistical quantity that measures chain convergence called the split r hat statistic. When when the chains have converged the between chain variation matches the within chain variation and the ratio approaches one. On the right we show the number of effective samples compared against the actual number of samples. Auto correlation between samples reduces the effective sample size and the non-center parameterization allows for more efficient exploration of the posterior by reducing autocorrelation.

Iteration three
While the previous model can Can show the differences between the pre and post adaptation blocks and the difference in age groups it does not consider any interaction between the two. implicitly the model assumes that temporary calibration affects all age groups in the same way. to model the interaction between categorical variables we create a new categorical variable that is the cross product between all the levels in each of the categories. the age group has levels young middle and older and the block has levels pre and post so the new interaction variable will six levels showing all combinations.

Iteration four lapse rate
a lapse in judgment can happen for any reason and is assumed to be random and independent of other lapses. an example of a lapse in judgment might be when a subject is being presented with the two stimuli and blinks or sneezes during the presentation. Alternatively the subject might not be paying attention and that is also considered a lapse in judgment. We can consider the total possibilities that a subject can experience when performing a temporal order judgment task. at the start of the trial the subject either experiences a lapse in judgment with probability gamma or they do not if there is no lapse then they will give a positive response with probability determined by the psychometric function. if there is a labs in judgment then it is assumed that they will respond randomly, so 50/50 chance of giving a positive or negative response. The probability of a positive response is the sum of the two paths.

Charts of a lapse rate model
The psychometric function is bounded by an upper and lower value determined by the lapse rate. This allows for estimated slopes to be very large while still allowing for some variation at larger temporal delays. Here we really see the benefit of using a flexible modeling platform like Stan. The model in the language of Stan is almost a one-to-one correspondence to the mathematical model.

Results of modeling a lapse rate
many of the subjects in the visual temporal order  judgments has had exceptional performance, but lapses in judgments still happen. this results in a conflict between the psychometric function trying to estimate a very steep slope while also allowing for some uncertainty at larger temporal delays. Modeling a lapse rate allows the psychometric function to reconcile these two opposing forces. The figure here shows the posterior distribution of psychometric functions for the older age group in the visual task. before adding a lapse rate it is difficult to determine if temporal recalibration has any effect on perceptual synchrony or temporal sensitivity. after modeling the lapse rate there's a clear distinction between the pre and post adaptation blocks.

Iteration 5
The final iteration of our model sees the addition of subjects level parameters. While we are primarily interested in making inferences at the age group level It is beneficial to be able to model individual subjects to make finer comparisons. The model presented here is the full Bayesian model that we use to estimate the posterior. It is the culmination of incorporating prior information to build a multi-level model with categorical interactions, non centered parameterization, and lapse rates.

Model assessment
because we are modeling or building our model in the context of a principal Bayesian workflow we must go back and answer the four questions for which to evaluate or model by for domain expertise consistency the model does not predict anything that conflicts with the main expertise for computational faithfulness the posterior samples are reliable as determined by posterior checks which include trace plots the split our hat the effective number of samples and divergent transitions for inferential adequacy modeling age and block interactions allows us to answer questions about age and temporal recalibration and for model adequacy modeling elapse rate is more true to the data generating process and improves predictive performance.

Perceptual synchrony
To keep things brief we present the temporal binding window results for the audiovisual temporal order judgment task. this plot breaks up the posterior distribution of estimated point of subjective simultaneity by both age group and block. on the left we can see that across each age group temporal recalibration resulted in a negative shift in perceptual synchrony. we can say that temporary calibration has a measurable effect on the point of subjective simultaneity but not a practical effect. The right hand of the figure shows that there's no visually significant difference between the different age groups.

Temporal sensitivity
The distribution of just noticeable differences is plotted similarly to the perceptual synchrony plots on the previous slide. We use the left hand figure to make comparisons within age groups, and the right hand figure to make comparisons within blocks. There's some evidence to suggest that the older age group has reduced temporal sensitivity, but temporal recalibration Can significantly increase their temporal sensitivity. Temple run calibration does not affect each age group in the same way. It appears to have less of a benefit for the younger age group and more for the older two.

Lapses
There's no unanimous visual trend in the lapse rates meaning that no single age group definitely experiences a lower lapse rate than the others though the middle age group comes close to being the winner and the older age group is more likely to be trailing behind. The distribution of lapse rates does reveal something about the tasks themselves. The distribution of lapse rates is more concentrated for the visual task compared to the other three. detecting temporal order of two visual stimuli maybe an easier mental task than that of heterogeneous signals. The subjects may perceive the different tasks as having different difficulties and so the latent measure of task difficulty might be picked up through the lapse rate.

Subject specific
The final iteration of the multi-level model provides subject-specific estimation as well as the age group level estimations presented in the previous three sliders. The left hand figure highlights the variation within age groups by comparing two subjects from the same task and age group. if we recall back to the data quirks slide one of the subjects Post that application block results were unreasonable and thrown out of the data set. However we still had their pre adaptation responses and so we can estimate what their post adaptation block performance would be.

Improving
The results here are based off of the small scale preliminary dataset, so there's a lot of uncertainty in the measure of the psychometric quantities. there's also an inherent difficulty for the different tasks where the visual is easier than the audio visual is easier than the duration is easier than the sensory motor. The relative difficulty is measured through the temporal sensitivity and the lapse rates within age groups. The lapses should be independent of the task but the distribution of the lapse rates across the ages and tasks shows that this assumption is violated. The psychometric experiment could be improved by presenting The subjects with larger temporal delays. The reasoning is that if the difficulty of a task is lowered then an incorrect response is more likely to be due to a true lapse in judgment as opposed to a genuinely and correct response. Wichmann and Hill recommends at least one sample at a performance level greater than or equal to 95% to reliably estimate confidence intervals. for the visual temporal order judgment task the 90% level may occur at a temporal delay of 40 milliseconds while for the audio visual temporal judgment that temporal delay may need to be as high as 220 milliseconds so the sampling scheme for psychometric experiments must be tuned to the task. finally in the visual temporal judgment task the granularity in the temporal delays near the point of subjective simultaneity could be increased to get more reliable estimates of the slope and to avoid complete separation.
