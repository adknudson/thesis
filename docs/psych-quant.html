<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Modeling Psychometric Quantities | A Bayesian Multilevel Model for the Psychometric Function using R and Stan</title>
  <meta name="description" content="4 Modeling Psychometric Quantities | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Modeling Psychometric Quantities | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="adkudson/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Modeling Psychometric Quantities | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  
  
  

<meta name="author" content="Alexander D. Knudson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="methods.html"/>
<link rel="next" href="discussion-and-conclusion.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#conventional-classical-statistics"><i class="fa fa-check"></i><b>1.1</b> Conventional (classical) statistics</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#markov-chain-monte-carlo-enables-modern-bayesian-models"><i class="fa fa-check"></i><b>1.3</b> Markov Chain Monte Carlo enables modern Bayesian models</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i><b>1.4</b> Organization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Motivating data</a><ul>
<li class="chapter" data-level="2.1" data-path="data.html"><a href="data.html#psycho-experiments"><i class="fa fa-check"></i><b>2.1</b> Psychometric experiments</a></li>
<li class="chapter" data-level="2.2" data-path="data.html"><a href="data.html#toj-task"><i class="fa fa-check"></i><b>2.2</b> Temporal order judgment tasks</a></li>
<li class="chapter" data-level="2.3" data-path="data.html"><a href="data.html#glms"><i class="fa fa-check"></i><b>2.3</b> Fitting the psychometric function using GLMs</a></li>
<li class="chapter" data-level="2.4" data-path="data.html"><a href="data.html#data-visualization-and-quirks"><i class="fa fa-check"></i><b>2.4</b> Data visualization and quirks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Background</a><ul>
<li class="chapter" data-level="3.1" data-path="methods.html"><a href="methods.html#bayesian-single-level-glm"><i class="fa fa-check"></i><b>3.1</b> Bayesian single-level GLM</a></li>
<li class="chapter" data-level="3.2" data-path="methods.html"><a href="methods.html#bayesian-multilevel-glm"><i class="fa fa-check"></i><b>3.2</b> Bayesian multilevel GLM</a></li>
<li class="chapter" data-level="3.3" data-path="methods.html"><a href="methods.html#a-modern-principled-bayesian-modeling-workflow"><i class="fa fa-check"></i><b>3.3</b> A modern principled bayesian modeling workflow</a></li>
<li class="chapter" data-level="3.4" data-path="methods.html"><a href="methods.html#model-checking"><i class="fa fa-check"></i><b>3.4</b> Methods for model checking</a></li>
<li class="chapter" data-level="3.5" data-path="methods.html"><a href="methods.html#estimating-predictive-performance"><i class="fa fa-check"></i><b>3.5</b> Estimating predictive performance</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="psych-quant.html"><a href="psych-quant.html"><i class="fa fa-check"></i><b>4</b> Modeling Psychometric Quantities</a></li>
<li class="chapter" data-level="5" data-path="discussion-and-conclusion.html"><a href="discussion-and-conclusion.html"><i class="fa fa-check"></i><b>5</b> Discussion and Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="code.html"><a href="code.html"><i class="fa fa-check"></i><b>A</b> Supplementary Code</a></li>
<li class="chapter" data-level="B" data-path="model-dev.html"><a href="model-dev.html"><i class="fa fa-check"></i><b>B</b> Developing a Model</a></li>
<li class="chapter" data-level="C" data-path="reproduce.html"><a href="reproduce.html"><i class="fa fa-check"></i><b>C</b> Reproducible Results</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Bayesian Multilevel Model for the Psychometric Function using R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="psych-quant" class="section level1">
<h1><span class="header-section-number">4</span> Modeling Psychometric Quantities</h1>
<!--
It is now time to define priors for the model, while still not having looked at the data. The priors should be motivated by domain expertise and *prior knowledge*, not the data. There are also many choices when it comes to selecting a psychometric (sigmoid) function. Common choices are logistic, Gaussian, and Weibull.
-->
<div class="figure" style="text-align: center"><span id="fig:ch040-pf-assortment"></span>
<img src="040-psychometric-quantities_files/figure-html/ch040-pf-assortment-1.png" alt="Assortment of psychometric functions." width="85%" />
<p class="caption">
Figure 4.1: Assortment of psychometric functions.
</p>
</div>
<p>The Weibull psychometric function is more common when it comes to 2-alternative forced choice (2-AFC) psychometric experiments where the independent variable is a stimulus intensity (non-negative) and the goal is signal detection. The data in this paper includes both positive and negative SOA values, so the Weibull is not a natural choice. Our first choice is the logistic function as it is the canonical choice for Binomial count data. The data in this study are exchangeable, meaning that the label of a positive response can be swapped with the label of a negative response and the inferences would remain the same. Since there is no natural ordering, it makes more sense for the psychometric function to be symmetric, e.g. the logistic function and Gaussian CDF. We use symmetric loosely to mean that the probability density function (PDF) has zero skewness. In practice, there is little difference in inferences between the <em>logit</em> and <em>probit</em> links, but computationally the logit link is more efficient.</p>
<p>It is appropriate to provide additional background to GLMs and their role in working with psychometric functions. A GLM allows the linear model to be related to the outcome variable via a <em>link</em> function. An example of this is the logit link – the inverse of the logistic function. The logistic function, <span class="math inline">\(F\)</span>, takes <span class="math inline">\(x \in \mathbb{R}\)</span> and constrains the output to be in <span class="math inline">\((0, 1)\)</span>.</p>
<p><span class="math display" id="eq:logistic">\[\begin{equation}
  F(\theta) = \frac{1}{1 + \exp\left(-\theta\right)}
  \tag{4.1}
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(F\)</span> is a strictly increasing and continuous function, it has an inverse, and the link for <a href="psych-quant.html#eq:logistic">(4.1)</a> is the log-odds or logit function.</p>
<p><span class="math display" id="eq:logit">\[\begin{equation}
  F^{-1}(\pi) = \mathrm{logit}(\pi) = \ln\left(\frac{\pi}{1 - \pi}\right)
  \tag{4.2}
\end{equation}\]</span></p>
<p>By taking <span class="math inline">\((F^{-1} \circ F)(\theta)\)</span> we can arrive at a relationship that is linear in <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[\begin{align*}
  \pi = F(\theta) \Longleftrightarrow F^{-1}(\pi) &amp;= F^{-1}(F(\theta)) \\
  &amp; = \ln\left(\frac{F(\theta)}{1 - F(\theta)}\right) \\
  &amp;= \ln(F(\theta)) - \ln(1 - F(\theta)) \\
  &amp;= \ln\left(\frac{1}{1 + \exp(-\theta)}\right) - \ln\left(\frac{\exp(-\theta)}{1 + \exp(-\theta)}\right) \\
  &amp;= - \ln(1 + \exp(-\theta)) - \ln(\exp(-\theta)) + \ln(1 + \exp(-\theta)) \\
  &amp;= - \ln(\exp(-\theta)) \\
  &amp;= \theta
\end{align*}\]</span></p>
<p>The motivation for this background is to show that a model for the psychometric function can be specified using a linear predictor, <span class="math inline">\(\theta\)</span>. Given a simple slope-intercept model, the linear predictor would typically be written as:</p>
<p><span class="math display" id="eq:linearform1">\[\begin{equation}
  \theta = \alpha + \beta x
  \tag{4.3}
\end{equation}\]</span></p>
<p>This isn’t the only possible form; it could be written in the slope-location parameterization:</p>
<p><span class="math display" id="eq:linearform2">\[\begin{equation}
  \theta = \beta(x - a)
  \tag{4.4}
\end{equation}\]</span></p>
<p>Both parameterizations will describe the same geometry, so why should it matter which form is chosen? The interpretation of the parameters change between the two models, but the reason becomes clear when we consider how the linear model relates back to the physical properties that the psychometric model describes. Take equation <a href="psych-quant.html#eq:linearform1">(4.3)</a>, substitute it in to <a href="psych-quant.html#eq:logistic">(4.1)</a>, and then take the logit of both sides:</p>
<p><span class="math display" id="eq:pfform1">\[\begin{equation}
  \mathrm{logit}(\pi) = \alpha+\beta x
  \tag{4.5}
\end{equation}\]</span></p>
<p>Now recall that the PSS is defined as the SOA value such that the response probability, <span class="math inline">\(\pi\)</span>, is <span class="math inline">\(0.5\)</span>. Substituting <span class="math inline">\(\pi = 0.5\)</span> into <a href="psych-quant.html#eq:pfform1">(4.5)</a> and solving for <span class="math inline">\(x\)</span> yields:</p>
<p><span class="math display">\[pss = -\frac{\alpha}{\beta}\]</span></p>
<p>Similarly, the JND is defined as the difference between the SOA value at the 84% level and the PSS. Substituting <span class="math inline">\(\pi = 0.84\)</span> into <a href="psych-quant.html#eq:pfform1">(4.5)</a>, solving for <span class="math inline">\(x\)</span>, and subtracting off the pss yields:</p>
<p><span class="math display" id="eq:jnd1">\[\begin{equation}
  jnd = \frac{\mathrm{logit}(0.84)}{\beta}
  \tag{4.6}
\end{equation}\]</span></p>
<p>From the conceptual analysis, it is easy to define priors for the PSS and JND, but then how does one set the priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>? Let’s say the prior for the just noticeable difference is <span class="math inline">\(jnd \sim \pi_j\)</span>. Then the prior for <span class="math inline">\(\beta\)</span> would be</p>
<p><span class="math display">\[\beta \sim \frac{\mathrm{logit}(0.84)}{\pi_j}\]</span></p>
<p>The log-normal distribution has a nice property where its multiplicative inverse is still a log-normal distribution. If we let <span class="math inline">\(\pi_j = \mathrm{Lognormal}(\mu, \sigma^2)\)</span>, then <span class="math inline">\(\beta\)</span> would be distributed as</p>
<p><span class="math display">\[
\beta \sim \mathrm{Lognormal}(-\mu + \ln(\mathrm{logit}(0.84)), \sigma^2)
\]</span></p>
<p>This is acceptable as the slope must always be positive for this psychometric data, and a log-normal distribution constrains the support to positive real numbers. Next suppose that the prior distribution for the PSS is <span class="math inline">\(pss \sim \pi_p\)</span>. Then the prior for <span class="math inline">\(\alpha\)</span> is:</p>
<p><span class="math display">\[\alpha \sim -\pi_p \cdot \beta\]</span></p>
<p>If <span class="math inline">\(\pi_p\)</span> is set to a log-normal distribution as well, then <span class="math inline">\(\pi_p \cdot \beta\)</span> would also be log-normal, but there is still the problem of the negative sign. If <span class="math inline">\(\alpha\)</span> is always negative, then the PSS will also always be negative, which is certainly not always true. Furthermore, we don’t want to <em>a priori</em> put more weight on positive PSS values compared to negative ones.</p>
<p>Let’s now consider using equation <a href="psych-quant.html#eq:linearform2">(4.4)</a> and repeat the above process.</p>
<p><span class="math display" id="eq:pfform2">\[\begin{equation}
  \mathrm{logit}(\pi) = \beta(x - a)
  \tag{4.7}
\end{equation}\]</span></p>
<p>The just noticeable difference is still given by <a href="psych-quant.html#eq:jnd1">(4.6)</a>, and so the same method for choosing a prior can be used. However, the PSS is now given by:</p>
<p><span class="math display">\[pss = \alpha\]</span></p>
<p>This is a fortunate consequence of using <a href="psych-quant.html#eq:linearform2">(4.4)</a> because now the JND only depends on <span class="math inline">\(\beta\)</span> and the PSS only depends on <span class="math inline">\(\alpha\)</span>. Additionally <span class="math inline">\(\alpha\)</span> can be interpreted as the PSS of the estimated psychometric function. Also thrown in is the ability to set a prior for <span class="math inline">\(\alpha\)</span> that is symmetric around <span class="math inline">\(0\)</span> such as a Gaussian distribution.</p>
<p>This also highlights the benefit of using a modeling language like <code>Stan</code> over others. For fitting GLMs in <code>R</code>, there are a handful of functions that utilize MLE like <code>stats::glm</code> and others that use Bayesian methods like <code>rstanarm::stan_glm</code> and <code>arm::bayesglm</code> <span class="citation">(Gabry and Goodrich <a href="#ref-R-rstanarm" role="doc-biblioref">2020</a>; Gelman and Su <a href="#ref-R-arm" role="doc-biblioref">2020</a>)</span>. Each of these functions requires the linear predictor to be in the form of <a href="psych-quant.html#eq:linearform1">(4.3)</a>. The <code>stan_glm</code> function uses Stan in the back-end to fit a model, but is limited to priors from the Student-t family of distributions. By writing the model directly in <code>Stan</code>, the linear model can be parameterized in any way and with any prior distribution, and so allows for much more expressive modeling.</p>

</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-rstanarm">
<p>Gabry, Jonah, and Ben Goodrich. 2020. <em>Rstanarm: Bayesian Applied Regression Modeling via Stan</em>. <a href="https://CRAN.R-project.org/package=rstanarm">https://CRAN.R-project.org/package=rstanarm</a>.</p>
</div>
<div id="ref-R-arm">
<p>Gelman, Andrew, and Yu-Sung Su. 2020. <em>Arm: Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. <a href="https://CRAN.R-project.org/package=arm">https://CRAN.R-project.org/package=arm</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="discussion-and-conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/adknudson/thesis/blob/master/040-psychometric-quantities.Rmd",
"text": null
},
"download": ["adknudson-thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toc_depth": 3,
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
