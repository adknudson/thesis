<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Motivating data | A Bayesian Multilevel Model for the Psychometric Function using R and Stan</title>
  <meta name="description" content="3 Motivating data | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Motivating data | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="adkudson/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Motivating data | A Bayesian Multilevel Model for the Psychometric Function using R and Stan" />
  
  
  

<meta name="author" content="Alexander D. Knudson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="methods.html"/>
<link rel="next" href="application.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#conventional-classical-statistics"><i class="fa fa-check"></i><b>1.1</b> Conventional (classical) statistics</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#bayesian-statistics"><i class="fa fa-check"></i><b>1.2</b> Bayesian statistics</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#markov-chain-monte-carlo-enables-modern-bayesian-models"><i class="fa fa-check"></i><b>1.3</b> Markov Chain Monte Carlo enables modern Bayesian models</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i><b>1.4</b> Organization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2</b> Background</a><ul>
<li class="chapter" data-level="2.1" data-path="methods.html"><a href="methods.html#glms"><i class="fa fa-check"></i><b>2.1</b> Fitting the psychometric function using GLMs</a></li>
<li class="chapter" data-level="2.2" data-path="methods.html"><a href="methods.html#multilevel-modeling"><i class="fa fa-check"></i><b>2.2</b> Multilevel modeling</a></li>
<li class="chapter" data-level="2.3" data-path="methods.html"><a href="methods.html#hamiltonian-monte-carlo-and-nuts"><i class="fa fa-check"></i><b>2.3</b> Hamiltonian Monte Carlo and NUTS</a></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html#non-centered-parameterization"><i class="fa fa-check"></i><b>2.4</b> Non-centered parameterization</a></li>
<li class="chapter" data-level="2.5" data-path="methods.html"><a href="methods.html#model-checking"><i class="fa fa-check"></i><b>2.5</b> Methods for model checking</a></li>
<li class="chapter" data-level="2.6" data-path="methods.html"><a href="methods.html#estimating-predictive-performance"><i class="fa fa-check"></i><b>2.6</b> Estimating predictive performance</a></li>
<li class="chapter" data-level="2.7" data-path="methods.html"><a href="methods.html#a-modern-principled-bayesian-modeling-workflow"><i class="fa fa-check"></i><b>2.7</b> A modern principled bayesian modeling workflow</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> Motivating data</a><ul>
<li class="chapter" data-level="3.1" data-path="data.html"><a href="data.html#psycho-experiments"><i class="fa fa-check"></i><b>3.1</b> Psychometric experiments</a></li>
<li class="chapter" data-level="3.2" data-path="data.html"><a href="data.html#toj-task"><i class="fa fa-check"></i><b>3.2</b> Temporal order judgment tasks</a></li>
<li class="chapter" data-level="3.3" data-path="data.html"><a href="data.html#data-visualization-and-quirks"><i class="fa fa-check"></i><b>3.3</b> Data visualization and quirks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="application.html"><a href="application.html"><i class="fa fa-check"></i><b>4</b> Methodological Contributions via the Workflow</a><ul>
<li class="chapter" data-level="4.1" data-path="application.html"><a href="application.html#psych-quant"><i class="fa fa-check"></i><b>4.1</b> Modeling psychometric quantities</a></li>
<li class="chapter" data-level="4.2" data-path="application.html"><a href="application.html#iter1"><i class="fa fa-check"></i><b>4.2</b> Iteration 1: base model</a></li>
<li class="chapter" data-level="4.3" data-path="application.html"><a href="application.html#iter2"><i class="fa fa-check"></i><b>4.3</b> Iteration 2: adding age and block</a></li>
<li class="chapter" data-level="4.4" data-path="application.html"><a href="application.html#iter3"><i class="fa fa-check"></i><b>4.4</b> Iteration 3: adding age-block interaction</a></li>
<li class="chapter" data-level="4.5" data-path="application.html"><a href="application.html#iter4"><i class="fa fa-check"></i><b>4.5</b> Iteration 4: adding a lapse rate</a></li>
<li class="chapter" data-level="4.6" data-path="application.html"><a href="application.html#iter5"><i class="fa fa-check"></i><b>4.6</b> Iteration 5: adding subjects</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>5</b> Psychometric Results</a><ul>
<li class="chapter" data-level="5.1" data-path="results.html"><a href="results.html#on-perceptual-synchrony"><i class="fa fa-check"></i><b>5.1</b> On Perceptual Synchrony</a></li>
<li class="chapter" data-level="5.2" data-path="results.html"><a href="results.html#on-temporal-sensitivity"><i class="fa fa-check"></i><b>5.2</b> On Temporal Sensitivity</a></li>
<li class="chapter" data-level="5.3" data-path="results.html"><a href="results.html#lapse-rate-across-age-groups"><i class="fa fa-check"></i><b>5.3</b> Lapse Rate across Age Groups</a></li>
<li class="chapter" data-level="5.4" data-path="results.html"><a href="results.html#subject-specific-inferences"><i class="fa fa-check"></i><b>5.4</b> Subject specific inferences</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="discussion-and-conclusion.html"><a href="discussion-and-conclusion.html"><i class="fa fa-check"></i><b>6</b> Discussion and Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="code.html"><a href="code.html"><i class="fa fa-check"></i><b>A</b> Supplementary Code</a></li>
<li class="chapter" data-level="B" data-path="model-dev.html"><a href="model-dev.html"><i class="fa fa-check"></i><b>B</b> Developing a Model</a></li>
<li class="chapter" data-level="C" data-path="reproduce.html"><a href="reproduce.html"><i class="fa fa-check"></i><b>C</b> Reproducible Results</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Bayesian Multilevel Model for the Psychometric Function using R and Stan</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data" class="section level1">
<h1><span class="header-section-number">3</span> Motivating data</h1>
<p>This paper focuses on a type of psychometric experiment called a temporal order judgment (TOJ) experiment. If there are two distinct stimuli occurring nearly simultaneously then our brains will bind them into a single percept (perceive them as happening simultaneously). Compensation for small temporal differences is beneficial for coherent multisensory experiences, particularly in visual-speech synthesis as it is necessary to maintain an accurate representation of the sources of multisensory events.</p>
<p>It was Charles Darwin who in his book <em>On the Origin of Species</em> developed the idea that living organisms adapt in order to better survive in their environment. Sir Francis Galton, inspired by Darwin’s ideas, became interested in the differences in human beings and in how to measure those differences. Galton’s works on studying and measuring human differences lead to the creation of psychometrics – the science of measuring mental faculties. Around the same time that he was developing his theories, Johann Friedrich Herbart was also interested in studying consciousness through the scientific method, and is responsible for creating mathematical models of the mind.</p>
<p>E.H. Weber built upon Herbart’s work, and sought out to prove the idea of a psychological threshold. A psychological threshold is a minimum stimulus intensity necessary to activate a sensory system – a liminal stimulus. He paved the way for experimental psychology and is the namesake of Weber’s Law (Equation <a href="data.html#eq:webers-law">(3.1)</a>), which states that the change in a stimulus that will be just noticeable is a constant ratio of the original stimulus <span class="citation">(Britannica <a href="#ref-britannica2014editors" role="doc-biblioref">2014</a>)</span>.</p>
<p><span class="math display" id="eq:webers-law">\[\begin{equation}
  \frac{\Delta I}{I} = k
  \tag{3.1}
\end{equation}\]</span></p>
<p>To demonstrate this law, consider holding a 1 kg weight (<span class="math inline">\(I = 1\)</span>), and further suppose that the difference between a 1 kg weight and a 1.2 kg weight (<span class="math inline">\(\Delta I = 0.2\)</span>) can just be detected. Then the constant just noticeable ratio is:</p>
<p><span class="math display">\[k = \frac{0.2}{1} = 0.2\]</span></p>
<p>Now consider picking up a 10 kg weight. The mass required to just detect a difference can be calculated as:</p>
<p><span class="math display">\[\frac{\Delta I}{10} = 0.2 \Rightarrow \Delta I = 2\]</span></p>
<p>The difference between a 10 kg and a 12 kg weight is expected to be just barely perceptible. Note that the difference in the first set of weights is 0.2, and in the second set it is 2. The perception of the difference in stimulus intensities is not absolute, but relative. G.T. Fechner devised the law (Weber-Fechner Law, Equation <a href="data.html#eq:weber-fechner-law">(3.2)</a>) that the strength of a sensation grows as the logarithm of the stimulus intensity.</p>
<p><span class="math display" id="eq:weber-fechner-law">\[\begin{equation}
  S = K \ln I
  \tag{3.2}
\end{equation}\]</span></p>
<p>Consider two light sources: one that is 100 lumens (<span class="math inline">\(S_1 = K \ln 100\)</span>) and another that is 200 lumens (<span class="math inline">\(S_2 = K \ln 200\)</span>). The intensity of the second light is not perceived as twice as bright, but only about 1.15 times as bright according to <a href="data.html#eq:weber-fechner-law">(3.2)</a>:</p>
<p><span class="math display">\[\theta = S_2 / S_1 \approx 1.15\]</span></p>
<p>Notice that the value <span class="math inline">\(K\)</span> cancels out when calculating the relative intensity, but knowing <span class="math inline">\(K\)</span> can lead to important psychological insights about differences between persons or groups of people. What biological and contextual factors affect how people perceive different stimuli? How do we measure their perception in a meaningful way? We can collect data from psychometric experiments, fit a model to the data from a family of functions called psychometric functions, and inspect key operating characteristics of those functions.</p>
<div id="psycho-experiments" class="section level2">
<h2><span class="header-section-number">3.1</span> Psychometric experiments</h2>
<p>Psychometric experiments are devised in a way to examine psychophysical processes, or the response between the world around us and our inward perceptions. A psychometric function relates an observer’s performance to an independent variable, usually some physical quantity of a stimulus in a psychophysical task <span class="citation">(Wichmann and Hill <a href="#ref-wichmann2001a" role="doc-biblioref">2001</a><a href="#ref-wichmann2001a" role="doc-biblioref">a</a>)</span>. Psychometric functions were studied as early as the late 1800’s, and Edwin Boring published a chart of the psychometric function in The American Journal of Psychology in 1917 <span class="citation">(Boring <a href="#ref-boring1917chart" role="doc-biblioref">1917</a>)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch020-chart-of-pf"></span>
<img src="figures/chart_of_pf.png" alt="A chart of the psychometric function. The experiment in this paper places two points on a subject's skin separated by some distance, and has them answer their impression of whether there is one point or two, recorded as either 'two points' or 'not two points'. As the separation of aesthesiometer points increases, so too does the subject's confidence in their perception of 'two-ness'. So at what separation is the impression of two points liminal?" width="85%" />
<p class="caption">
Figure 3.1: A chart of the psychometric function. The experiment in this paper places two points on a subject’s skin separated by some distance, and has them answer their impression of whether there is one point or two, recorded as either ‘two points’ or ‘not two points’. As the separation of aesthesiometer points increases, so too does the subject’s confidence in their perception of ‘two-ness’. So at what separation is the impression of two points liminal?
</p>
</div>
<p>Figure <a href="data.html#fig:ch020-chart-of-pf">3.1</a> displays the key aspects of the psychometric function. The most crucial part is the sigmoid function, the S-like non-decreasing curve which in this case is represented by the Normal CDF, <span class="math inline">\(\Phi(\gamma)\)</span>. The horizontal axis represents the stimulus intensity: the separation of two points in centimeters. The vertical axis represents the probability that a subject has the impression of two points. With only experimental data, the response proportion becomes an approximation for the probability.</p>
<p>The temporal asynchrony between stimuli is called the stimulus onset asynchrony (SOA), and the range of SOAs for which sensory signals are integrated into a global percept is called the temporal binding window. When the SOA grows large enough, the brain segregates the two signals and the temporal order can be determined.</p>
<p>Our experiences in life as we age shape the mechanisms of processing multisensory signals, and some multisensory signals are integrated much more readily than others. Perceptual synchrony has been previously studied through the point of subjective simultaneity (PSS) – the temporal delay between two signals at which an observer is unsure about their temporal order <span class="citation">(Stone et al. <a href="#ref-stone2001now" role="doc-biblioref">2001</a>)</span>. The temporal binding window is the time span over which sensory signals arising from different modalities appear integrated into a global percept.</p>
<p>A deficit in temporal sensitivity may lead to a widening of the temporal binding window and reduce the ability to segregate unrelated sensory signals. In TOJ tasks, the ability to discriminate the timing of multiple sensory signals is referred to as temporal sensitivity, and is studied through the measurement of the just noticeable difference (JND) – the smallest lapse in time so that a temporal order can just be determined.</p>
<p>Figure <a href="data.html#fig:ch020-plot-ref-pf">3.2</a> highlights the features through which we study psychometric functions. The PSS is defined as the point where an observer can do no better at determining temporal order than random guessing (i.e. when the response probability is 50%). The JND is defined as the extra temporal delay between stimuli so that the temporal order is just able to be determined. Historically this has been defined as the difference between the 84% level – one standard deviation away from the mean – and the PSS, though the upper level often depends on domain expertise.</p>
<div class="figure" style="text-align: center"><span id="fig:ch020-plot-ref-pf"></span>
<img src="020-psychometrics_files/figure-html/ch020-plot-ref-pf-1.png" alt="The PSS is defined as the point where an observer can do no better at determining temporal order than random guessing. The just noticeable difference is defined as the extra temporal delay between stimuli so that the temporal order is just able to be determined. Historically this has been defined as the difference between the 0.84 level and the PSS, though the upper level depends on domain expertise." width="85%" />
<p class="caption">
Figure 3.2: The PSS is defined as the point where an observer can do no better at determining temporal order than random guessing. The just noticeable difference is defined as the extra temporal delay between stimuli so that the temporal order is just able to be determined. Historically this has been defined as the difference between the 0.84 level and the PSS, though the upper level depends on domain expertise.
</p>
</div>
<p>Perceptual synchrony and temporal sensitivity can be modified through a baseline understanding. In order to perceive physical events as simultaneous, our brains must adjust for differences in temporal delays of transmission of both psychical signals and sensory processing <span class="citation">(Fujisaki et al. <a href="#ref-fujisaki2004recalibration" role="doc-biblioref">2004</a>)</span>. In some cases such as with audiovisual stimuli, the perception of simultaneity can be modified by repeatedly presenting the audiovisual stimuli at fixed time separations (called an adapter stimulus) to an observer <span class="citation">(Vroomen et al. <a href="#ref-vroomen2004recalibration" role="doc-biblioref">2004</a>)</span>. This repetition of presenting the adapter stimulus is called temporal recalibration.</p>
</div>
<div id="toj-task" class="section level2">
<h2><span class="header-section-number">3.2</span> Temporal order judgment tasks</h2>
<p>The data set used in this paper comes from small-scale preliminary experiments done by A.N. Scurry and Dr. Jiang in the Department of Psychology at the University of Nevada. Reduced temporal sensitivity in the aging population manifests in an impaired ability to perceive synchronous events as simultaneous, and similarly more difficulty in segregating asynchronous sensory signals that belong to different sources. The consequences of a widening of the temporal binding window is considered in <span class="citation">Scurry et al. (<a href="#ref-scurry2019aging" role="doc-biblioref">2019</a>)</span>, as well as a complete detailing of the experimental setup and recording process. Here we present a shortened summary of the experimental methods.</p>
<p>There are four different tasks in the experiment: audio-visual, visual-visual, visual-motor, and duration, and each task is respectively referred to as audiovisual, visual, sensorimotor, and duration. The participants consist of 15 young adults (age 20-27), 15 middle age adults (age 39-50), and 15 older adults (age 65-75), all recruited from the University of Nevada, Reno. Additionally all subjects are right handed and are reported to have normal or corrected to normal hearing and vision.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ch020-multitask-data">Table 3.1: </span>Sample of motivating data.
</caption>
<thead>
<tr>
<th style="text-align:right;">
soa
</th>
<th style="text-align:right;">
response
</th>
<th style="text-align:left;">
sid
</th>
<th style="text-align:left;">
task
</th>
<th style="text-align:left;">
trial
</th>
<th style="text-align:left;">
age_group
</th>
<th style="text-align:right;">
age
</th>
<th style="text-align:left;">
sex
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-350
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
O-m-BC
</td>
<td style="text-align:left;">
audiovisual
</td>
<td style="text-align:left;">
pre
</td>
<td style="text-align:left;">
older_adult
</td>
<td style="text-align:right;">
70
</td>
<td style="text-align:left;">
M
</td>
</tr>
<tr>
<td style="text-align:right;">
-200
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
M-m-SJ
</td>
<td style="text-align:left;">
duration
</td>
<td style="text-align:left;">
post1
</td>
<td style="text-align:left;">
middle_age
</td>
<td style="text-align:right;">
48
</td>
<td style="text-align:left;">
M
</td>
</tr>
<tr>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
O-f-KK
</td>
<td style="text-align:left;">
sensorimotor
</td>
<td style="text-align:left;">
pre
</td>
<td style="text-align:left;">
older_adult
</td>
<td style="text-align:right;">
66
</td>
<td style="text-align:left;">
F
</td>
</tr>
<tr>
<td style="text-align:right;">
275
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
O-f-MW
</td>
<td style="text-align:left;">
visual
</td>
<td style="text-align:left;">
post1
</td>
<td style="text-align:left;">
older_adult
</td>
<td style="text-align:right;">
69
</td>
<td style="text-align:left;">
F
</td>
</tr>
</tbody>
</table>
<p>In the audiovisual TOJ task, participants were asked to determine the temporal order between an auditory and visual stimulus. Stimulus onset asynchrony values were selected uniformly between -500 to +500 ms with 50 ms steps, where negative SOAs indicated that the visual stimulus was leading, and positive values indicated that the auditory stimulus was leading. Each SOA value was presented 5 times in random order in the initial block. At the end of each trial the subject was asked to report if the auditory stimulus came before the visual, where a <span class="math inline">\(1\)</span> indicates that they perceived the sound first, and a <span class="math inline">\(0\)</span> indicates that they perceived the visual stimulus first.</p>
<p>A similar setup is repeated for the visual, sensorimotor, and duration tasks. The visual task presented two visual stimuli on the left and right side of a display with temporal asynchronies that varied between -300 ms to +300 ms with 25 ms steps. Negative SOAs indicated that the left stimulus was first, and positive that the right came first. A positive response indicates that the subject perceived the right stimulus first.</p>
<p>The sensorimotor task has subjects focus on a black cross on a screen. When it disappears, they respond by pressing a button. Additionally, when the cross disappears, a visual stimulus was flashed on the screen, and subjects were asked if they perceived the visual stimulus before or after their button press. The latency of the visual stimulus was partially determined by individual subject’s average response time, so SOA values are not fixed between subjects and trials. A positive response indicates that the visual stimulus was perceived after the button press.</p>
<p>The duration task presents two vertically stacked circles on a screen with one appearing right after the other. The top stimulus appeared for a fixed amount of time of 300 ms, and the bottom was displayed for anywhere between +100 ms to +500 ms in 50 ms steps corresponding to SOA values between -200 ms to +200 ms. The subject then responds to if they perceived the bottom circle as appearing longer than the top circle.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ch020-toj-summary">Table 3.2: </span>Summary of TOJ Tasks
</caption>
<thead>
<tr>
<th style="text-align:left;">
Task
</th>
<th style="text-align:left;">
Positive Response
</th>
<th style="text-align:left;">
Positive SOA Truth
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Audiovisual
</td>
<td style="text-align:left;">
Perceived audio first
</td>
<td style="text-align:left;">
Audio came before visual
</td>
</tr>
<tr>
<td style="text-align:left;">
Visual
</td>
<td style="text-align:left;">
Perceived right first
</td>
<td style="text-align:left;">
Right came before left
</td>
</tr>
<tr>
<td style="text-align:left;">
Sensorimotor
</td>
<td style="text-align:left;">
Perceived visual first
</td>
<td style="text-align:left;">
Visual came before tactile
</td>
</tr>
<tr>
<td style="text-align:left;">
Duration
</td>
<td style="text-align:left;">
Perceived bottom as longer
</td>
<td style="text-align:left;">
Bottom lasted longer than top
</td>
</tr>
</tbody>
</table>
<p>After the first block of each task was completed, the participants went through an adaptation period where they were presented with the respective stimuli from each task repeatedly at fixed temporal delays, then the TOJ task was repeated. To ensure that the adaptation affect persisted, the subject was presented with the adapter stimulus at regular intervals throughout the second block. The blocks are designated as <code>pre</code> and <code>post1</code>, <code>post2</code>, etc. in the data set. In this paper we only focus on the <code>pre</code> and <code>post1</code> blocks.</p>
</div>
<div id="data-visualization-and-quirks" class="section level2">
<h2><span class="header-section-number">3.3</span> Data visualization and quirks</h2>
<p>The dependent variable in these TOJ experiments is the subject’s perceived response, encoded as a 0 or a 1, and the independent variable is the SOA value. If the response is plotted against the SOA values, then it is difficult to determine the relationship (see the right panel of figure <a href="data.html#fig:ch020-simple-response-soa-plot">3.3</a>). Transparency can be used to better visualize the relationship. The center panel in figure <a href="data.html#fig:ch020-simple-response-soa-plot">3.3</a> shows the same data as the left, except that the transparency is set to <span class="math inline">\(0.05\)</span>. Note that there is a higher density of “0” responses towards more negative SOAs, and a higher density of “1” responses for more positive SOAs. The proportion of “positive” responses for a given SOA is computed and plotted against the SOA value (displayed in the right panel of figure <a href="data.html#fig:ch020-simple-response-soa-plot">3.3</a>). The relationship between SOA values and responses is clear – as the SOA value goes from more negative to more positive, the proportion of positive responses increases from near 0 to near 1.</p>
<div class="figure" style="text-align: center"><span id="fig:ch020-simple-response-soa-plot"></span>
<img src="020-psychometrics_files/figure-html/ch020-simple-response-soa-plot-1.png" alt="Left: Simple plot of response vs. soa value. Center: A plot of response vs. soa with transparency. Right: A plot of proportions vs. soa with transparency." width="85%" />
<p class="caption">
Figure 3.3: Left: Simple plot of response vs. soa value. Center: A plot of response vs. soa with transparency. Right: A plot of proportions vs. soa with transparency.
</p>
</div>
<p>The right panel in figure <a href="data.html#fig:ch020-simple-response-soa-plot">3.3</a> is the easiest to interpret, and we often present the observed and predicted data using the proportion of responses rather than the raw responses. Proportional data is also bounded on the same interval as the response in contrast to the raw counts.</p>
<p>For the audiovisual task, the responses can be aggregated into binomial data – the number of positive responses for given SOA value – which is more efficient to work with than the Bernoulli data (see table <a href="data.html#tab:ch020-av-bin-sample">3.3</a>). However the number of times an SOA is presented varies between the pre-adaptation and post-adaptation blocks; 5 and 3 times per SOA respectively.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ch020-av-bin-sample">Table 3.3: </span>Audiovisual task with aggregated responses.
</caption>
<thead>
<tr>
<th style="text-align:left;">
trial
</th>
<th style="text-align:right;">
soa
</th>
<th style="text-align:right;">
n
</th>
<th style="text-align:right;">
k
</th>
<th style="text-align:right;">
proportion
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
pre
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.80
</td>
</tr>
<tr>
<td style="text-align:right;">
150
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:right;">
-350
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:left;vertical-align: top !important;" rowspan="3">
post1
</td>
<td style="text-align:right;">
350
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1.00
</td>
</tr>
<tr>
<td style="text-align:right;">
-500
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.33
</td>
</tr>
<tr>
<td style="text-align:right;">
-200
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
</tbody>
</table>
<p>There is one younger subject that did not complete the audiovisual task, and one younger subject that did not complete the duration task. There is also one older subject who’s response data for the post-adaptation audiovisual task is unreasonable – it is extremely unlikely that the data represents genuine responses (see figure <a href="data.html#fig:ch020-av-post1-O-f-CE-plot">3.4</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:ch020-av-post1-O-f-CE-plot"></span>
<img src="020-psychometrics_files/figure-html/ch020-av-post1-O-f-CE-plot-1.png" alt="Post-adaptation response data for O-f-CE" width="85%" />
<p class="caption">
Figure 3.4: Post-adaptation response data for O-f-CE
</p>
</div>
<p>Of all the negative SOAs, there were only two “correct” responses (the perceived order matches the actual order). If a subject is randomly guessing the temporal order, then a naive estimate for the proportion of correct responses is 0.5. If a subject’s proportion of correct responses is above 0.5, then they are doing better than random guessing. Figure <a href="data.html#fig:ch020-av-post-neg-trials">3.5</a> shows that subject <code>O-f-CE</code> is the only one who’s proportion is below 0.5 (and by a considerable amount), and so their post-adaptation block is removed from data set for model fitting.</p>
<div class="figure" style="text-align: center"><span id="fig:ch020-av-post-neg-trials"></span>
<img src="020-psychometrics_files/figure-html/ch020-av-post-neg-trials-1.png" alt="Proportion of correct responses for negative SOA values during the post-adaptation audiovisual experiment." width="85%" />
<p class="caption">
Figure 3.5: Proportion of correct responses for negative SOA values during the post-adaptation audiovisual experiment.
</p>
</div>
<!-- I may want to move some of this to Results

When this method of detecting outliers is repeated for all tasks and blocks, then we end up with 17 records in total (figure \@ref(fig:ch020-naive-prop-outliers)), one of which is the aforementioned subject.


<div class="figure" style="text-align: center">
<img src="020-psychometrics_files/figure-html/ch020-naive-prop-outliers-1.png" alt="Proportion of correct responses across all tasks and blocks Proportions are calculated individually for positive and negative SOAs." width="85%" />
<p class="caption">(\#fig:ch020-naive-prop-outliers)Proportion of correct responses across all tasks and blocks Proportions are calculated individually for positive and negative SOAs.</p>
</div>


Most of the records that are flagged by this method of outlier detection are from the sensorimotor task, and none are from the visual task. This may be attributed to the perceived difficulty of the task. One consequence of higher temporal sensitivity is that it is easier to determine temporal order. It may also be that determining temporal order is inherently easier for certain multisensory tasks compared to others. Since the sensorimotor task does not have fixed SOA values like the other tasks, it may be perceived as more difficult. Or perhaps the mechanisms that process tactile and visual signals are not as well coupled as those that process audio and visual signals.

-->

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boring1917chart">
<p>Boring, Edwin G. 1917. “A Chart of the Psychometric Function.” <em>The American Journal of Psychology</em> 28 (4): 465–70.</p>
</div>
<div id="ref-britannica2014editors">
<p>Britannica, Encyclopaedia. 2014. “The Editors of Encyclopaedia Britannica.” Dispersion.</p>
</div>
<div id="ref-fujisaki2004recalibration">
<p>Fujisaki, Waka, Shinsuke Shimojo, Makio Kashino, and Shin’ya Nishida. 2004. “Recalibration of Audiovisual Simultaneity.” <em>Nature Neuroscience</em> 7 (7): 773–78.</p>
</div>
<div id="ref-scurry2019aging">
<p>Scurry, Alexandra N, Tiziana Vercillo, Alexis Nicholson, Michael Webster, and Fang Jiang. 2019. “Aging Impairs Temporal Sensitivity, but Not Perceptual Synchrony, Across Modalities.” <em>Multisensory Research</em> 32 (8): 671–92.</p>
</div>
<div id="ref-stone2001now">
<p>Stone, JV, NM Hunkin, J Porrill, R Wood, V Keeler, M Beanland, M Port, and NR Porter. 2001. “When Is Now? Perception of Simultaneity.” <em>Proceedings of the Royal Society of London. Series B: Biological Sciences</em> 268 (1462): 31–38.</p>
</div>
<div id="ref-vroomen2004recalibration">
<p>Vroomen, Jean, Mirjam Keetels, Beatrice De Gelder, and Paul Bertelson. 2004. “Recalibration of Temporal Order Perception by Exposure to Audio-Visual Asynchrony.” <em>Cognitive Brain Research</em> 22 (1): 32–35.</p>
</div>
<div id="ref-wichmann2001a">
<p>Wichmann, Felix A, and N Jeremy Hill. 2001a. “The Psychometric Function: I. Fitting, Sampling, and Goodness of Fit.” <em>Perception &amp; Psychophysics</em> 63 (8): 1293–1313.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="application.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/adknudson/thesis/blob/master/020-psychometrics.Rmd",
"text": null
},
"download": ["adknudson-thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toc_depth": 3,
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
