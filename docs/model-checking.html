<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Model Fitting/Checking | Application of a Principaled Bayesian Workflow to Multilevel Modeling</title>
  <meta name="description" content="4 Model Fitting/Checking | Application of a Principaled Bayesian Workflow to Multilevel Modeling" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Model Fitting/Checking | Application of a Principaled Bayesian Workflow to Multilevel Modeling" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="adkudson/thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Model Fitting/Checking | Application of a Principaled Bayesian Workflow to Multilevel Modeling" />
  
  
  

<meta name="author" content="Alexander D. Knudson" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="workflow.html"/>
<link rel="next" href="predictive-inferences.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#ch010-classical-methods"><i class="fa fa-check"></i><b>1.1</b> Everything can be Blamed on Fisher</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#ch010-new-methods"><i class="fa fa-check"></i><b>1.2</b> Proposal of New Methods</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#ch010-organization"><i class="fa fa-check"></i><b>1.3</b> Organization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="motivating-data.html"><a href="motivating-data.html"><i class="fa fa-check"></i><b>2</b> What is a Model without Data</a><ul>
<li class="chapter" data-level="2.1" data-path="motivating-data.html"><a href="motivating-data.html#psycho-experiments"><i class="fa fa-check"></i><b>2.1</b> Psychometric Experiments</a></li>
<li class="chapter" data-level="2.2" data-path="motivating-data.html"><a href="motivating-data.html#toj-task"><i class="fa fa-check"></i><b>2.2</b> Temporal Order Judgment Data</a></li>
<li class="chapter" data-level="2.3" data-path="motivating-data.html"><a href="motivating-data.html#data-visualizations-and-quirks"><i class="fa fa-check"></i><b>2.3</b> Data Visualizations and Quirks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>3</b> Principled Bayesian Workflow</a><ul>
<li class="chapter" data-level="3.1" data-path="workflow.html"><a href="workflow.html#iter1"><i class="fa fa-check"></i><b>3.1</b> Iteration 1 (journey of a thousand miles)</a></li>
<li class="chapter" data-level="3.2" data-path="workflow.html"><a href="workflow.html#iter2"><i class="fa fa-check"></i><b>3.2</b> Iteration 2 (electric boogaloo)</a></li>
<li class="chapter" data-level="3.3" data-path="workflow.html"><a href="workflow.html#iter3"><i class="fa fa-check"></i><b>3.3</b> Iteration 3 (the one for me)</a></li>
<li class="chapter" data-level="3.4" data-path="workflow.html"><a href="workflow.html#iter4"><i class="fa fa-check"></i><b>3.4</b> Iteration 4 (what’s one more)</a></li>
<li class="chapter" data-level="3.5" data-path="workflow.html"><a href="workflow.html#celebrate"><i class="fa fa-check"></i><b>3.5</b> Celebrate</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>4</b> Model Fitting/Checking</a><ul>
<li class="chapter" data-level="4.1" data-path="model-checking.html"><a href="model-checking.html#fitting-using-hmc"><i class="fa fa-check"></i><b>4.1</b> Fitting using HMC</a><ul>
<li class="chapter" data-level="4.1.1" data-path="model-checking.html"><a href="model-checking.html#diagnostic-tools"><i class="fa fa-check"></i><b>4.1.1</b> Diagnostic Tools</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-checking.html"><a href="model-checking.html#prior-predictive-checks"><i class="fa fa-check"></i><b>4.2</b> Prior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="predictive-inferences.html"><a href="predictive-inferences.html"><i class="fa fa-check"></i><b>5</b> Predictive Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="predictive-inferences.html"><a href="predictive-inferences.html#model-comparison-via-predictive-performance"><i class="fa fa-check"></i><b>5.1</b> Model Comparison via Predictive Performance</a><ul>
<li class="chapter" data-level="5.1.1" data-path="predictive-inferences.html"><a href="predictive-inferences.html#loocv-and-importance-sampling"><i class="fa fa-check"></i><b>5.1.1</b> LOOCV and Importance Sampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>6</b> Psychometric Results</a><ul>
<li class="chapter" data-level="6.1" data-path="results.html"><a href="results.html#affect-of-adaptation-across-age-groups"><i class="fa fa-check"></i><b>6.1</b> Affect of Adaptation across Age Groups</a><ul>
<li class="chapter" data-level="6.1.1" data-path="results.html"><a href="results.html#on-perceptual-synchrony"><i class="fa fa-check"></i><b>6.1.1</b> On Perceptual Synchrony</a></li>
<li class="chapter" data-level="6.1.2" data-path="results.html"><a href="results.html#on-temporal-sensitivity"><i class="fa fa-check"></i><b>6.1.2</b> On Temporal Sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="results.html"><a href="results.html#lapse-rate-across-age-groups"><i class="fa fa-check"></i><b>6.2</b> Lapse Rate across Age Groups</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>7</b> Discussion</a><ul>
<li class="chapter" data-level="7.1" data-path="discussion.html"><a href="discussion.html#model-selection-is-not-always-the-goal"><i class="fa fa-check"></i><b>7.1</b> Model selection is not always the goal</a></li>
<li class="chapter" data-level="7.2" data-path="discussion.html"><a href="discussion.html#data-cleaning-and-reproducibility"><i class="fa fa-check"></i><b>7.2</b> Data Cleaning and Reproducibility</a></li>
<li class="chapter" data-level="7.3" data-path="discussion.html"><a href="discussion.html#developing-a-model"><i class="fa fa-check"></i><b>7.3</b> Developing a model</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>8</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="supplementary-code.html"><a href="supplementary-code.html"><i class="fa fa-check"></i><b>A</b> Supplementary Code</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Application of a Principaled Bayesian Workflow to Multilevel Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-checking" class="section level1">
<h1><span class="header-section-number">4</span> Model Fitting/Checking</h1>
<p><em>Check your model before you wreck your model</em></p>
<p>This chapter serves as the formal home of definitions and explanations of concepts relating to Markov Chain Monte Carlo (MCMC) and other diagnostic tools when working with Bayesian inference models. I touched on the physics of Hamiltonian Monte Carlo (HMC) and the diagnostic tools that come with it in the previous chapter, but now I will go into more detail.</p>
<div id="fitting-using-hmc" class="section level2">
<h2><span class="header-section-number">4.1</span> Fitting using HMC</h2>
<p>Why do we need a sampler at all? Bayesian statistics and modeling stems from Bayes theorem (Equation <a href="model-checking.html#eq:bayesthm">(4.1)</a>). The prior <span class="math inline">\(P(\theta)\)</span> is some distribution over the parameter space and the likelihood <span class="math inline">\(P(X | \theta)\)</span> is the probability of an outcome in the sample space given a value in the parameter space. To keep things simple, we generally say that the posterior is proportional to the prior times the likelihood. Why proportional? The posterior distribution is a probability distribution, which means that the sum or integral over the parameter space must evaluate to one. Because of this constraint, the denominator in <a href="model-checking.html#eq:bayesthm">(4.1)</a> acts as a scale factor to ensure that the posterior is valid.</p>
<p><span class="math display" id="eq:bayesthm">\[\begin{equation}
  P(\theta | X) = \frac{P(X | \theta)\cdot P(\theta)}{\sum_i P(X | \theta_i)} =   \frac{P(X | \theta)\cdot P(\theta)}{\int_\Omega P(X | \theta)d\theta}
  \tag{4.1}
\end{equation}\]</span></p>
<p>For simple models, the posterior distribution can sometimes be evaluated analytically. An example of this is in <em>conjugate</em> models, where the resulting posterior distribution is of the same type as the prior distribution, and an example of a conjugate model is the Beta distribution for inference about a proportion statistic. This is common in baseball for a player’s batting average. I don’t know a lot about baseball, but I know that hitting a baseball is a little less common than one in three swings, so <em>a priori</em> I believe the probability of hitting a baseball is distributed as <span class="math inline">\(\mathrm{Beta}(2, 5)\)</span> because the expected value is <span class="math inline">\(\approx 0.29\)</span> and not a lot of weight is given to any particular value. Throughout a game I follow one player and he hits four balls and misses six - data that can be modeled as a Binomial observation. To figure out the posterior distribution for batting average, I use Bayes’ theorem - <em>posterior is proportional to the prior times the likelihood</em>.</p>
<p><span class="math display">\[\begin{align*}
  P(\pi | y) &amp;\propto P(y | \pi) \cdot P(\pi) \\
  &amp;= {10 \choose 4}\pi^{4} (1-\pi)^{6} \cdot \frac{\Gamma(2+5)}{\Gamma(2)\Gamma(5)} \pi^{2-1}(1-\pi)^{5-1} \\
  &amp;\propto \pi^{4+2-1}(1-\pi)^{6+5-1} \\
  &amp;= \pi^{6-1}(1-\pi)^{11-1}
\end{align*}\]</span></p>
<p>The final line is the shape of a Beta distribution with parameters <span class="math inline">\(6=2+4\)</span> and <span class="math inline">\(11=5+6\)</span>. The simple update rule is that for a prior <span class="math inline">\(\mathrm{Beta}(a, b)\)</span> and observed data with <span class="math inline">\(y\)</span> successes in <span class="math inline">\(n\)</span> observations, the posterior distribution is <span class="math inline">\(\mathrm{Beta}(a + y, b + n - y)\)</span>. For the baseball player, the Bayesian estimate of his batting average is <span class="math inline">\(6/(6+11) \approx 0.353\)</span>, but still with a good amount of uncertainty as shown in figure <a href="model-checking.html#fig:ch040-Teal-Monkey">4.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ch040-Teal-Monkey"></span>
<img src="040-model-checking_files/figure-html/ch040-Teal-Monkey-1.png" alt="After observing 4 hits in 10, the Beta(2,5) prior gets updated to become a Beta(6,11) posterior." width="85%" />
<p class="caption">
Figure 4.1: After observing 4 hits in 10, the Beta(2,5) prior gets updated to become a Beta(6,11) posterior.
</p>
</div>
<p>Conjugate models are great for simple observational data, but often it happens that the posterior distribution cannot be deduced from the model or that the integral in the denominator is complex or of a high dimension. In the former situation, the integral may not be possible to evaluate, and in the latter there may not be enough computational resources in the world to perform a simple grid approximation.</p>
<p>The solution is to use Markov Chain Monte Carlo (MCMC). The idea is that we can <em>draw samples</em> from the posterior distribution in a way that samples proportional to the density. This sampling is a form of approximation to the area under the curve (i.e. an approximation to the denominator in <a href="model-checking.html#eq:bayesthm">(4.1)</a>). Rejection sampling <span class="citation">(Gilks and Wild <a href="#ref-gilks1992adaptive" role="doc-biblioref">1992</a>)</span> and slice sampling <span class="citation">(Neal <a href="#ref-neal2003slice" role="doc-biblioref">2003</a>)</span> are basic methods for sampling from a target distribution, however they can often be inefficient<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. NUTS is a much more complex algorithm that can be compared to a physics simulation. A massless “particle” is flicked in a random direction with some amount of kinetic energy in a probability field, and is stopped randomly. The stopping point is the new proposal sample. The No U-Turn part means that when the algorithm detects that the particle is turning around, it will stop so as not to return to the starting position. This sampling scheme has a much higher rate of accepted samples, and also comes with many built-in diagnostic tools that let us know when the sampler is having trouble efficiently exploring the posterior. I’ll talk more about these diagnostic tools throughout the remaining sections.</p>
<div id="diagnostic-tools" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Diagnostic Tools</h3>
<div id="trace-plots" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Trace Plots</h4>
<p>Trace plots are the first line of defense against misbehaved samplers. They are visual aids that let the practitioner asses the qualitative health of the chains, looking for properties such as autocorrelation, heteroskedacity, non-stationarity, and convergence. Healthy chains are <em>well-mixing</em> and stationary. It’s often better to run more chains during the model building process so that issues with mixing and convergence can be diagnosed sooner. Even one unhealthy chain can be indicative of a poorly specified model. The addition of more chains also contributes to the estimation of the Split <span class="math inline">\(\hat{R}\)</span> statistic, which I discuss in <a href="model-checking.html#split-r">4.1.1.2</a>. Figure <a href="model-checking.html#fig:ch040-Brave-Moose">4.2</a> shows what a set of healthy chains looks like.</p>
<div class="figure" style="text-align: center"><span id="fig:ch040-Brave-Moose"></span>
<img src="040-model-checking_files/figure-html/ch040-Brave-Moose-1.png" alt="An example of healthy chains." width="85%" />
<p class="caption">
Figure 4.2: An example of healthy chains.
</p>
</div>
<p>There is a similar diagnostic plot called the rank histogram plot (or <em>trank</em> plot for trace rank plot). <span class="citation">Vehtari, Gelman, et al. (<a href="#ref-vehtari2020rank" role="doc-biblioref">2020</a>)</span> details the motivation for trank plots, but in short if the chains are all exploring the posterior efficiently, then the histograms will be similar and uniform. Figure <a href="model-checking.html#fig:ch040-Dog-Reborn">4.3</a> is from the same model as above but for the rank histogram.</p>
<div class="figure" style="text-align: center"><span id="fig:ch040-Dog-Reborn"></span>
<img src="040-model-checking_files/figure-html/ch040-Dog-Reborn-1.png" alt="A trank plot of healthy chains." width="85%" />
<p class="caption">
Figure 4.3: A trank plot of healthy chains.
</p>
</div>
<p>As the number of parameters in a model grows, it becomes exceedingly tedious to check the trace and trank plots of all parameters, and so numerical summaries are required to flag potential issues within the model.</p>
</div>
<div id="split-r" class="section level4">
<h4><span class="header-section-number">4.1.1.2</span> <span class="math inline">\(\hat{R}\)</span> and Split <span class="math inline">\(\hat{R}\)</span></h4>
<p>The most common summary statistic for chain health is the potential scale reduction factor <span class="citation">(Gelman, Rubin, and others <a href="#ref-gelman1992inference" role="doc-biblioref">1992</a>)</span> that measures the ratio of between chain variance and within chain variance. When the two have converged, the ratio is one. I’ve already shared examples of healthy chains which would also have healthy <span class="math inline">\(\hat{R}\)</span> values, but it’s valuable to also share an example of a bad model. Below is the 8 Schools example <span class="citation">(Gelman et al. <a href="#ref-gelman2013bayesian" role="doc-biblioref">2013</a>)</span> which is a classical example for introducing Stan and testing the operating characteristics of a model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="model-checking.html#cb22-1"></a>schools_dat &lt;-<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb22-2"><a href="model-checking.html#cb22-2"></a>  <span class="dt">J =</span> <span class="dv">8</span>,</span>
<span id="cb22-3"><a href="model-checking.html#cb22-3"></a>  <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">28</span>,  <span class="dv">8</span>, <span class="dv">-3</span>,  <span class="dv">7</span>, <span class="dv">-1</span>,  <span class="dv">1</span>, <span class="dv">18</span>, <span class="dv">12</span>),</span>
<span id="cb22-4"><a href="model-checking.html#cb22-4"></a>  <span class="dt">sigma =</span> <span class="kw">c</span>(<span class="dv">15</span>, <span class="dv">10</span>, <span class="dv">16</span>, <span class="dv">11</span>,  <span class="dv">9</span>, <span class="dv">11</span>, <span class="dv">10</span>, <span class="dv">18</span>)</span>
<span id="cb22-5"><a href="model-checking.html#cb22-5"></a>)</span></code></pre></div>
<p>The initial starting parameters for this model are intentionally set to vary between <span class="math inline">\(-10\)</span> and <span class="math inline">\(10\)</span> (in contrast to the default range of <span class="math inline">\((-2, 2)\)</span>) and with only a few samples drawn in order to artificially drive up the split <span class="math inline">\(\hat{R}\)</span> statistic. The model is provided as supplementary code in the <a href="supplementary-code.html#supplementary-code">appendix</a>.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="model-checking.html#cb23-1"></a>fit_cp &lt;-<span class="st"> </span><span class="kw">sampling</span>(schools_mod_cp, <span class="dt">data =</span> schools_dat, <span class="dt">refresh =</span> <span class="dv">0</span>,</span>
<span id="cb23-2"><a href="model-checking.html#cb23-2"></a>                   <span class="dt">iter =</span> <span class="dv">50</span>, <span class="dt">init_r =</span> <span class="dv">10</span>, <span class="dt">seed =</span> <span class="dv">671254821</span>)</span></code></pre></div>
<p>Stan instantly warns about many different issues with this model, but the R-hat is the one of interest. The largest is <span class="math inline">\(1.68\)</span> which is incredibly large</p>
<p><img src="040-model-checking_files/figure-html/ch040-Rocky-Test-1.png" width="85%" style="display: block; margin: auto;" /></p>
<p>These chains do not look good at all! Let’s take a look at the <span class="math inline">\(\hat{R}\)</span> values and see if we can calculate one of the values manually.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-1">Table 4.1: </span>Split R-hat values from the 8 Schools example.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Parameter
</th>
<th style="text-align:right;">
Rhat
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mu
</td>
<td style="text-align:right;">
1.234
</td>
</tr>
<tr>
<td style="text-align:left;">
tau
</td>
<td style="text-align:right;">
1.596
</td>
</tr>
</tbody>
</table>
<p>To calculate the (non split) <span class="math inline">\(\hat{R}\)</span>, first calculate the between-chain variance, and then the average chain variance. For <span class="math inline">\(M\)</span> independent Markov chains, <span class="math inline">\(\theta_m\)</span>, with <span class="math inline">\(N\)</span> samples each, the between-chain variance is</p>
<p><span class="math display">\[
B = \frac{N}{M-1}\sum_{m=1}^{M}\left(\bar{\theta}_m - \bar{\theta}\right)^2
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\bar{\theta}_m = \frac{1}{N}\sum_{n=1}^{N}\theta_{m}^{(n)}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\bar{\theta} = \frac{1}{M}\sum_{m=1}^{M}\bar{\theta}_m
\]</span></p>
<p>The within-chain variance, <span class="math inline">\(W\)</span>, is the variance averaged over all the chains.</p>
<p><span class="math display">\[
W = \frac{1}{M}\sum_{m=1}^{M} s_{m}^2
\]</span></p>
<p>where</p>
<p><span class="math display">\[
s_{m}^2 = \frac{1}{N-1}\sum_{n=1}^{N}\left(\theta_{m}^{(n)} - \bar{\theta}_m\right)^2
\]</span></p>
<p>The variance estimator is a weighted mixture of the within-chain and cross-chain variation</p>
<p><span class="math display">\[
\hat{var} = \frac{N-1}{N} W + \frac{1}{N} B
\]</span></p>
<p>and finally</p>
<p><span class="math display">\[
\hat{R} = \sqrt{\frac{\hat{var}}{W}}
\]</span></p>
<p>Here is the calculation in R</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="model-checking.html#cb24-1"></a>param &lt;-<span class="st"> &quot;mu&quot;</span></span>
<span id="cb24-2"><a href="model-checking.html#cb24-2"></a>theta &lt;-<span class="st"> </span>p_cp[,,param]</span>
<span id="cb24-3"><a href="model-checking.html#cb24-3"></a>N     &lt;-<span class="st"> </span><span class="kw">nrow</span>(theta)</span>
<span id="cb24-4"><a href="model-checking.html#cb24-4"></a>M     &lt;-<span class="st"> </span><span class="kw">ncol</span>(theta)</span>
<span id="cb24-5"><a href="model-checking.html#cb24-5"></a></span>
<span id="cb24-6"><a href="model-checking.html#cb24-6"></a>theta_bar_m &lt;-<span class="st"> </span><span class="kw">colMeans</span>(theta)</span>
<span id="cb24-7"><a href="model-checking.html#cb24-7"></a>theta_bar   &lt;-<span class="st"> </span><span class="kw">mean</span>(theta_bar_m)</span>
<span id="cb24-8"><a href="model-checking.html#cb24-8"></a></span>
<span id="cb24-9"><a href="model-checking.html#cb24-9"></a>B &lt;-<span class="st"> </span>N <span class="op">/</span><span class="st"> </span>(M <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((theta_bar_m <span class="op">-</span><span class="st"> </span>theta_bar)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb24-10"><a href="model-checking.html#cb24-10"></a>s_sq_m &lt;-<span class="st"> </span><span class="kw">apply</span>(theta, <span class="dv">2</span>, var)</span>
<span id="cb24-11"><a href="model-checking.html#cb24-11"></a></span>
<span id="cb24-12"><a href="model-checking.html#cb24-12"></a>W &lt;-<span class="st"> </span><span class="kw">mean</span>(s_sq_m)</span>
<span id="cb24-13"><a href="model-checking.html#cb24-13"></a>var_hat &lt;-<span class="st"> </span>W <span class="op">*</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>N <span class="op">+</span><span class="st"> </span>B <span class="op">/</span><span class="st"> </span>N</span>
<span id="cb24-14"><a href="model-checking.html#cb24-14"></a></span>
<span id="cb24-15"><a href="model-checking.html#cb24-15"></a>(mu_Rhat &lt;-<span class="st"> </span><span class="kw">sqrt</span>(var_hat <span class="op">/</span><span class="st"> </span>W))</span>
<span id="cb24-16"><a href="model-checking.html#cb24-16"></a><span class="co">#&gt; [1] 1.134</span></span></code></pre></div>
<p>The <span class="math inline">\(\hat{R}\)</span> statistic is smaller than the split <span class="math inline">\(\hat{R}\)</span> value provided by Stan. This is a consequence of steadily increasing or decreasing chains. The split value does what it sounds like, and splits the chains in half and measures each half separately. In this way, the measure is more robust in detecting unhealthy chains. This also highlights the utility in using both visual and statistical tools to evaluate models.</p>
</div>
<div id="effective-sample-size" class="section level4">
<h4><span class="header-section-number">4.1.1.3</span> Effective Sample Size</h4>
<p>Samples from Markov Chains are typically autocorrelated, which can increase uncertainty of posterior estimates. I encountered this issue in the <a href="workflow.html#iter2">second iteration</a> of the model building process, and the solution I used was to reparameterize the model to avoid steep log-posterior densities - the benefit of reparameterization is conveyed by the ratio of effective sample size to actual sample size in figure <a href="model-checking.html#fig:ch040-Timely-Nitrogen">4.4</a>. When the HMC algorithm is exploring difficult geometry, it can get stuck in regions of high densities, which means that there is more correlation between successive samples.</p>
<div class="figure" style="text-align: center"><span id="fig:ch040-Timely-Nitrogen"></span>
<img src="040-model-checking_files/figure-html/ch040-Timely-Nitrogen-1.png" alt="Ratio of N_eff to actual sample size. Low ratios imply high autocorrelation which can be alleviated by reparameterizing the model or by thinning." width="85%" />
<p class="caption">
Figure 4.4: Ratio of N_eff to actual sample size. Low ratios imply high autocorrelation which can be alleviated by reparameterizing the model or by thinning.
</p>
</div>
<p>As the strength of autocorrelation generally decreases at larger lags, a simple prescription to decrease autocorrelation between samples and increase the effective sample size is to use <em>thinning</em>. Thinning means saving every <span class="math inline">\(k^{th}\)</span> sample and throwing the rest away. If one desired to have 2000 posterior draws, it could be done in two of many possible ways</p>
<ul>
<li>Generate 2000 draws after warmup and save all of them</li>
<li>Generate 10,000 draws after warmup and save every <span class="math inline">\(5^{th}\)</span> sample.</li>
</ul>
<p>Both will produce 2000 samples, but the method using thinning will have less autocorrelation and a higher effective number of samples. Though it should be noted that generating 10,000 draws and saving all of them will have a higher number of effective samples than the second method with thinning, so thinning should only be favored to save memory.</p>
</div>
<div id="divergent-transitions" class="section level4">
<h4><span class="header-section-number">4.1.1.4</span> Divergent Transitions</h4>
<p>Unlike the previous tools for algorithmic faithfulness which can be used for any MCMC sampler, information about divergent transitions is intrinsic to Hamiltonian Monte Carlo. Recall that the HMC and NUTS algorithm can be imagined as a physics simulation of a particle in a potential energy field, and a random momentum is imparted on the particle. The sum of the potential energy and the kinetic energy of the system is called the Hamiltonian, and is conserved along the trajectory of the particle <span class="citation">(Stan Development Team <a href="#ref-stanref" role="doc-biblioref">2020</a>)</span>. The path that the particle takes is a discrete approximation to the actual path where the position of the particle is updated in small steps called <em>leapfrog steps</em> (see <span class="citation">Leimkuhler and Reich (<a href="#ref-leimkuhler2004simulating" role="doc-biblioref">2004</a>)</span> for a detailed explanation of the leapfrog algorithm). A divergent transition happens when the simulated trajectory is far from the true trajectory as measured by the Hamiltonian.</p>
<p>A few divergent transitions is not indicative of a poorly performing model, and often divergent transitions can be reduced by reducing the step size and increasing the adapt delta parameter. On the other hand, a bad model may never be improved just by tweaking some parameters. This is the folk theorem of statistical computing - if there is a problem with the sampling, blame the model, not the algorithm.</p>
<p>Divergent transitions are never saved in the posterior samples, but they are saved internally to the Stan fit object and can be compared against good samples. Sometimes this can give insight into which parameters and which regions of the posterior the divergent transitions are coming from.</p>
<div class="figure" style="text-align: center"><span id="fig:ch040-Hot-Locomotive"></span>
<img src="040-model-checking_files/figure-html/ch040-Hot-Locomotive-1.png" alt="Divergent transitions highlighted for some parameters from the second iteration model. Divergent transitions tend to occur when both the hierarchical variance terms are near zero." width="85%" />
<p class="caption">
Figure 4.5: Divergent transitions highlighted for some parameters from the second iteration model. Divergent transitions tend to occur when both the hierarchical variance terms are near zero.
</p>
</div>
</div>
</div>
</div>
<div id="prior-predictive-checks" class="section level2">
<h2><span class="header-section-number">4.2</span> Prior Predictive Checks</h2>
<p>I used prior predictive checks in the first iteration of the model to establish a few things pertaining to model adequacy and computational faithfulness. The first reason is to ensure that the selected priors do not put too much mass in completely implausible regions (such as really large JND estimates). Data simulated from the priors can also be used to check that the software works. When you have the exact priors that were used to generate the data, the fitting algorithm should be able to accurately recover the priors.</p>
<ul>
<li>transition to posterior predictive checks chapter</li>
<li>fig 10 in for posterior predictive <span class="citation">Gabry et al. (<a href="#ref-gabry2019visualization" role="doc-biblioref">2019</a>)</span></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-gabry2019visualization">
<p>Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. “Visualization in Bayesian Workflow.” <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 182 (2): 389–402.</p>
</div>
<div id="ref-gelman2013bayesian">
<p>Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. <em>Bayesian Data Analysis</em>. CRC press.</p>
</div>
<div id="ref-gelman1992inference">
<p>Gelman, Andrew, Donald B Rubin, and others. 1992. “Inference from Iterative Simulation Using Multiple Sequences.” <em>Statistical Science</em> 7 (4): 457–72.</p>
</div>
<div id="ref-gilks1992adaptive">
<p>Gilks, Walter R, and Pascal Wild. 1992. “Adaptive Rejection Sampling for Gibbs Sampling.” <em>Journal of the Royal Statistical Society: Series C (Applied Statistics)</em> 41 (2): 337–48.</p>
</div>
<div id="ref-leimkuhler2004simulating">
<p>Leimkuhler, Benedict, and Sebastian Reich. 2004. <em>Simulating Hamiltonian Dynamics</em>. Vol. 14. Cambridge university press.</p>
</div>
<div id="ref-neal2003slice">
<p>Neal, Radford M. 2003. “Slice Sampling.” <em>Annals of Statistics</em>, 705–41.</p>
</div>
<div id="ref-stanref">
<p>Stan Development Team. 2020. <em>Stan Modeling Language Users Guide and Reference Manual</em>. <a href="https://mc-stan.org">https://mc-stan.org</a>.</p>
</div>
<div id="ref-vehtari2020rank">
<p>Vehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul-Christian Bürkner, and others. 2020. “Rank-Normalization, Folding, and Localization: An Improved R-Hat for Assessing Convergence of Mcmc.” <em>Bayesian Analysis</em>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>Efficiency of a sampler is related to the proportion of proposal samples that get accepted.<a href="model-checking.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="workflow.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="predictive-inferences.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/adknudson/thesis/blob/master/040-model-checking.Rmd",
"text": null
},
"download": ["adknudson-thesis.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toc_depth": 3,
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
